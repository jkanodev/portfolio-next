"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[734],{7495:(e,t,r)=>{r.d(t,{A:()=>a});var o=r(5155),s=r(853),n=r(9602);function a(e){let{value:t,onChange:r,placeholder:a="Search errors, commands, projectsâ€¦",className:i,...c}=e;return(0,o.jsxs)("div",{className:(0,n.cn)("relative",i),children:[(0,o.jsx)(s.A,{className:"absolute left-3 top-1/2 h-4 w-4 -translate-y-1/2 text-[var(--text-muted)]","aria-hidden":!0}),(0,o.jsx)("input",{type:"search",value:t,onChange:r,placeholder:a,className:"w-full rounded-lg border border-[var(--border)] bg-[var(--surface)] py-2.5 pl-10 pr-4 text-[var(--text)] placeholder:text-[var(--text-muted)] focus:border-[var(--accent)] focus:outline-none focus:ring-1 focus:ring-[var(--accent)]","aria-label":"Search",...c})]})}},5132:(e,t,r)=>{r.d(t,{$R:()=>a,eu:()=>s,gK:()=>n});var o=r(8426);function s(e){var t;let r=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{keys:["title","summary","tags","category","one_liner"],threshold:.35};return new o.A(e,{keys:r.keys,threshold:null!==(t=r.threshold)&&void 0!==t?t:.35,includeScore:!0})}function n(e,t){return t.trim()?e.search(t.trim()):[]}let a=s},6987:(e,t,r)=>{r.d(t,{Lr:()=>u,Nv:()=>f,OT:()=>d,Sr:()=>l,U:()=>h,ds:()=>m,lP:()=>g,oh:()=>c,v7:()=>p});let o="jkano_voice",s="jkano_speech_rate",n="jkano_speech_pitch",a=null;function i(){return a||(a=window.speechSynthesis),a}function c(){let e=i();if(!e)return Promise.resolve([]);let t=e.getVoices();return t.length>0?Promise.resolve(t):new Promise(t=>{e.onvoiceschanged=()=>t(e.getVoices())})}function l(){return{voiceName:localStorage.getItem(o)||"",rate:parseFloat(localStorage.getItem(s)||"1"),pitch:parseFloat(localStorage.getItem(n)||"1")}}function d(e,t,r){localStorage.setItem(o,e),localStorage.setItem(s,String(t)),localStorage.setItem(n,String(r))}function u(e){var t,r;let o=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},s=i();if(!s||!e.trim())return;s.cancel();let n=new SpeechSynthesisUtterance(e);n.rate=null!==(t=o.rate)&&void 0!==t?t:l().rate,n.pitch=null!==(r=o.pitch)&&void 0!==r?r:l().pitch;let a=s.getVoices(),c=o.voiceName?a.find(e=>e.name===o.voiceName)||a[0]:a.find(e=>e.name===l().voiceName)||a[0];c&&(n.voice=c),s.speak(n)}function p(){let e=i();e&&e.pause()}function h(){let e=i();e&&e.resume()}function m(){let e=i();e&&e.cancel()}function f(){let e=i();return!!e&&e.speaking}function g(){let e=i();return!!e&&e.paused}},9602:(e,t,r)=>{function o(){for(var e=arguments.length,t=Array(e),r=0;r<e;r++)t[r]=arguments[r];return t.filter(Boolean).join(" ")}function s(e){var t,r,o;if(null===(t=e.narration)||void 0===t?void 0:t.trim())return e.narration;let s=[];return e.title&&s.push(e.title+"."),e.summary&&s.push(e.summary),(null===(r=e.symptoms)||void 0===r?void 0:r.length)&&s.push("Symptoms: "+e.symptoms.join(". ")),e.root_cause&&s.push("Root cause: "+e.root_cause),(null===(o=e.fix_steps)||void 0===o?void 0:o.length)&&s.push("Fix: "+e.fix_steps.map(e=>e.step+". "+e.why).join(" ")),s.join(" ")||"No narration available."}r.d(t,{cn:()=>o,h:()=>s})},3461:e=>{e.exports=JSON.parse('[{"id":"cloudflare-cache-stale-site","title":"Site showed old version after Docker update","category":"Cloudflare","tags":["cache","deployment","stale-content"],"summary":"Origin updated but visitors still saw older content due to edge caching.","narration":"This one was annoying. I rebuilt the container and restarted it, but the public site still looked old. The fix was not Docker. It was Cloudflare caching the old version at the edge. Purge the cache, then verify with curl headers so you know the edge is pulling fresh content.","symptoms":["Site looked old even after docker build and restart","Local container looked correct but public site did not change"],"root_cause":"Cloudflare edge caching served stale content after origin changed.","fix_steps":[{"step":"Purge Cloudflare cache","why":"Forces edge nodes to fetch fresh content from origin."},{"step":"Restart cloudflared if needed","why":"Refreshes tunnel process and reconnection."}],"commands":[{"cmd":"docker ps","note":"Confirm containers running."},{"cmd":"sudo systemctl restart cloudflared","note":"Refresh tunnel service."},{"cmd":"curl -I https://your-domain-here","note":"Verify headers and status."}],"verification_steps":["Hard refresh and confirm new content","curl shows expected headers and status"],"prevention":["Use sane cache headers for HTML","Prefer immutable asset filenames","Purge cache on deploy when needed"],"related_entries":["cloudflare-purge-fixed-old-site","docker-old-site-still-serving"],"date_context":"2026-02"},{"id":"docker-restart-no-such-container-web","title":"docker restart web failed with No such container","category":"Docker","tags":["naming","lifecycle"],"summary":"Tried restarting a container name that did not exist.","narration":"I ran docker restart web and it yelled no such container. That means the name is wrong or the container is gone. First list containers, then restart the real name. If it does not exist, recreate it.","symptoms":["docker restart web cloudflared failed","No such container: web"],"root_cause":"Wrong container name or container was removed.","fix_steps":[{"step":"Run docker ps -a to find real names","why":"You need the exact container name or id."},{"step":"Restart correct containers","why":"Restart only works for existing containers."},{"step":"Recreate missing container with docker run or compose","why":"Removed containers must be recreated."}],"commands":[{"cmd":"docker ps -a","note":"See real names."},{"cmd":"docker restart portfolio cloudflared","note":"Restart correct containers."}],"verification_steps":["docker ps shows running","curl responds"],"prevention":["Standardize names","Use compose for stable service names"],"related_entries":["docker-old-site-still-serving"],"date_context":"2026-02"},{"id":"docker-old-site-still-serving","title":"Website still served old content after rebuild","category":"Docker","tags":["build","deploy","stale"],"summary":"Rebuild happened but the running container was not replaced correctly.","narration":"Sometimes the issue is not Cloudflare. Sometimes you rebuilt the image but never actually replaced the running container. The fix is to stop, remove, and run the new container cleanly. Then verify with curl.","symptoms":["docker build ran","site unchanged"],"root_cause":"Old container still running or new image not deployed.","fix_steps":[{"step":"Stop the running container","why":"You cannot swap what is still running on the port."},{"step":"Remove the container","why":"Forces a clean run with the new image."},{"step":"Run the container again from the rebuilt image","why":"Guarantees the new version is live."}],"commands":[{"cmd":"docker build -t portfolio .","note":"Build new image."},{"cmd":"docker stop portfolio || true","note":"Stop old container."},{"cmd":"docker rm portfolio || true","note":"Remove old container."},{"cmd":"docker run -d -p 80:80 --name portfolio portfolio","note":"Run new container."}],"verification_steps":["docker ps shows new container running","curl shows expected content"],"prevention":["Use a deploy script","Tag images clearly"],"related_entries":["cloudflare-cache-stale-site"],"date_context":"2026-02"},{"id":"ingress-served-nginx-welcome-page","title":"Ingress kept serving nginx welcome page instead of my app","category":"Kubernetes","tags":["ingress","default-namespace","routing"],"summary":"Leftover demo workloads intercepted routing and returned nginx default page.","narration":"Ingress was not broken. My cluster was dirty. There were leftover nginx demo resources in default namespace, so routing got hijacked. The fix was deleting the demo, deleting old ingress objects, then verifying the real service and host mapping.","symptoms":["host returned nginx welcome","app service existed but not served"],"root_cause":"Leftover nginx demo resources in default namespace.","fix_steps":[{"step":"List pods services deploy across namespaces","why":"Find what is still alive."},{"step":"Delete nginx demo resources in default","why":"Remove traffic hijacker."},{"step":"Delete old ingress objects","why":"Remove stale routing rules."},{"step":"Verify service backend and curl host","why":"End to end proof."}],"commands":[{"cmd":"kubectl get pods -A","note":"Find leftovers."},{"cmd":"kubectl get svc -A","note":"Find conflicts."},{"cmd":"kubectl get deploy -A","note":"Find demo deploy."},{"cmd":"kubectl get ingress -A","note":"Find old rules."}],"verification_steps":["curl shows app not nginx","describe ingress shows correct backend"],"prevention":["Keep demos out of default","Namespace discipline","Clean before testing"],"related_entries":["leftover-default-namespace-resources"],"date_context":"2025-09"},{"id":"leftover-default-namespace-resources","title":"Default namespace leftovers caused confusing behavior","category":"Kubernetes","tags":["namespace","cleanup"],"summary":"Old demo resources in default namespace interfered with current work.","narration":"Default namespace is where confusion lives. When old demo stuff stays there, it overrides what you are testing. The fix is listing everything and deleting the leftovers.","symptoms":["unexpected routing","unexpected services","unexpected pods"],"root_cause":"Old resources remained in default namespace.","fix_steps":[{"step":"List all resources across namespaces","why":"You need full visibility."},{"step":"Delete demo deployments services ingresses","why":"Clear the cluster."}],"commands":[{"cmd":"kubectl get all -n default","note":"Inspect default."},{"cmd":"kubectl get pods -A","note":"Cluster wide view."}],"verification_steps":["default namespace is clean","routing matches expected"],"prevention":["Never build real stuff in default","Use namespaces per project"],"related_entries":["ingress-served-nginx-welcome-page"],"date_context":"2025-09"},{"id":"github-actions-ssh-timeout-port-22","title":"GitHub Actions SSH deploy timed out","category":"CI-CD","tags":["ssh","timeout","router","ufw"],"summary":"Runner could not reach SSH due to networking or secret configuration.","narration":"The runner could not dial port 22. That can be wrong HOST secret, router port forwarding, firewall, or ISP blocking. The fix is verifying each layer like a chain. Secrets, router, firewall, then alternate path like Tailscale.","symptoms":["dial tcp i o timeout in GitHub Actions"],"root_cause":"Port 22 not reachable or wrong endpoint.","fix_steps":[{"step":"Verify HOST secret is correct","why":"Runner must dial the correct target."},{"step":"Verify router port forwarding for 22","why":"Inbound must reach server."},{"step":"Verify UFW allows 22 tcp","why":"Server must accept SSH."},{"step":"If ISP blocks 22, use Tailscale SSH approach","why":"No inbound port exposure and more reliable."}],"commands":[{"cmd":"sudo ufw status","note":"Check firewall."},{"cmd":"sudo ufw allow 22/tcp","note":"Allow SSH."}],"verification_steps":["Manual SSH from another network works","Pipeline completes"],"prevention":["Prefer Tailscale or tunnels for SSH deploy"],"related_entries":["host-secret-missing-or-wrong","router-port-forwarding-mismatch","ufw-blocked-ssh"],"date_context":"2026-02"},{"id":"host-secret-missing-or-wrong","title":"GitHub Actions HOST secret missing or wrong","category":"CI-CD","tags":["secrets","github-actions"],"summary":"Deploy failed because the target host value was incorrect or unset.","narration":"If HOST is wrong, nothing else matters. The runner is dialing the wrong place. Fix the secret first, then test again.","symptoms":["pipeline cannot reach host","timeouts"],"root_cause":"HOST secret not set or incorrect.","fix_steps":[{"step":"Set the correct HOST in GitHub Secrets","why":"Pipeline needs correct endpoint."}],"commands":[],"verification_steps":["Pipeline reaches correct host"],"prevention":["Document secrets required for pipeline"],"related_entries":["github-actions-ssh-timeout-port-22"],"date_context":"2026-02"},{"id":"ufw-blocked-ssh","title":"UFW blocked SSH until rule was added","category":"Linux","tags":["ufw","ssh"],"summary":"Firewall rules prevented inbound SSH.","narration":"You can forward ports perfectly and still fail if UFW blocks it. Allow 22 tcp and confirm status.","symptoms":["SSH times out","ports forwarded but no connection"],"root_cause":"UFW did not allow 22 tcp.","fix_steps":[{"step":"Allow 22 tcp in UFW","why":"Opens SSH port."},{"step":"Check UFW status","why":"Confirm rule exists and is active."}],"commands":[{"cmd":"sudo ufw allow 22/tcp","note":"Allow SSH."},{"cmd":"sudo ufw status","note":"Confirm rules."}],"verification_steps":["SSH works"],"prevention":["Add firewall rules during server bootstrap"],"related_entries":["github-actions-ssh-timeout-port-22"],"date_context":"2026-02"},{"id":"cloudflared-service-running-but-site-wrong","title":"cloudflared is running but the site still looks wrong","category":"Cloudflare","tags":["tunnel","routing","verification"],"summary":"Tunnel is active but content served does not match expectation.","narration":"cloudflared being active does not prove your site is the version you think. You still need to verify origin content and public content. Use docker ps, curl, and logs. Then purge cache if needed.","symptoms":["systemctl shows cloudflared active","site content mismatch"],"root_cause":"Tunnel health is separate from application version and caching.","fix_steps":[{"step":"Verify portfolio container is healthy","why":"Tunnel can be fine while the app is wrong."},{"step":"Check cloudflared logs","why":"Confirms tunnel connectivity and routing behavior."},{"step":"curl your domain and compare","why":"Proves what users actually see."}],"commands":[{"cmd":"sudo systemctl status cloudflared","note":"Check service state."},{"cmd":"docker ps","note":"Confirm portfolio container."},{"cmd":"docker logs cloudflared --tail 200","note":"Inspect tunnel logs."},{"cmd":"curl -I https://your-domain-here","note":"Headers check."}],"verification_steps":["curl matches expected content","container health is healthy"],"prevention":["Always verify with curl after deploy","Document what changed per deploy"],"related_entries":["cloudflare-cache-stale-site"],"date_context":"2026-02"},{"id":"cloudflare-purge-fixed-old-site","title":"Purging Cloudflare cache fixed the old site instantly","category":"Cloudflare","tags":["cache","postmortem"],"summary":"Root cause confirmed as edge caching, not build or container.","narration":"This is the confirmation entry. We proved the container was updated, then purged cache, then the site updated instantly. That is how you know it was Cloudflare edge cache.","symptoms":["site updated only after purge"],"root_cause":"Edge cache held old HTML.","fix_steps":[{"step":"Purge cache","why":"Invalidates edge."},{"step":"Verify with curl headers","why":"Confirms edge response changed."}],"commands":[{"cmd":"curl -I https://your-domain-here","note":"Confirm response."}],"verification_steps":["site shows new version"],"prevention":["Cache control strategy for HTML"],"related_entries":["cloudflare-cache-stale-site"],"date_context":"2026-02"},{"id":"kubectl-run-alreadyexists-pod-test","title":"kubectl run test returned AlreadyExists","category":"Kubernetes","tags":["debug","pods"],"summary":"A previous debug pod name already existed.","narration":"I tried to run a debug pod named test and Kubernetes said already exists. That means the old debug pod is still there. Delete it or use a new name like test-2.","symptoms":["AlreadyExists error for pod test"],"root_cause":"pod test already existed from previous debug.","fix_steps":[{"step":"Delete the existing pod","why":"Frees the name and cleans leftovers."},{"step":"Re run with unique name","why":"Avoids collisions."}],"commands":[{"cmd":"kubectl delete pod test","note":"Delete it."},{"cmd":"kubectl run test-2 --image=busybox -it --rm -- wget -qO- http://web","note":"Unique debug name."}],"verification_steps":["debug command succeeds"],"prevention":["Unique debug naming","Clean up if --rm does not run"],"related_entries":[],"date_context":"2026-02"},{"id":"curl-placeholder-host","title":"curl failed because I used a placeholder host","category":"Networking","tags":["curl","placeholders"],"summary":"curl failed because YOUR_PUBLIC_IP was never replaced.","narration":"This was a classic. The command had YOUR_PUBLIC_IP and I ran it like that. curl cannot resolve a placeholder. Replace it with the real value then retry.","symptoms":["Could not resolve host: YOUR_PUBLIC_IP"],"root_cause":"Placeholder string used as a real host.","fix_steps":[{"step":"Replace placeholder with real domain or IP","why":"Placeholders are not resolvable."},{"step":"Use curl -I for quick headers verification","why":"Fast check after deploy."}],"commands":[{"cmd":"curl -I https://your-domain-here","note":"Headers only."}],"verification_steps":["curl returns status and headers"],"prevention":["Use {{TOKENS}} in docs so replacements are obvious"],"related_entries":[],"date_context":"2026-02"},{"id":"git-no-upstream-branch","title":"git push failed because branch had no upstream","category":"Git","tags":["upstream","remote"],"summary":"git push failed because main was not tracking origin main.","narration":"git said no upstream branch. That means my local branch was not tracking the remote. The fix is one command and it stays fixed for future pushes.","symptoms":["The current branch main has no upstream branch"],"root_cause":"No tracking branch set for origin.","fix_steps":[{"step":"Set upstream on push","why":"Creates tracking so future pushes work normally."}],"commands":[{"cmd":"git push --set-upstream origin main","note":"Set upstream and push."}],"verification_steps":["Next git push works without flags"],"prevention":["Set upstream on first push"],"related_entries":[],"date_context":"2026-02"},{"id":"router-port-forwarding-mismatch","title":"Router port forwarding did not match server LAN IP","category":"Networking","tags":["router","ports"],"summary":"Inbound ports were not forwarded to the correct internal host.","narration":"Port forwarding is simple but brutal. If WAN 22 or 80 points at the wrong LAN IP, it fails. Fix the mapping and test from outside the network.","symptoms":["SSH unreachable from outside","site unreachable on public IP"],"root_cause":"Router forwarding rules incorrect.","fix_steps":[{"step":"Confirm server LAN IP then update forwarding rules","why":"Rules must target the correct host."}],"commands":[],"verification_steps":["External test reaches server"],"prevention":["Reserve LAN IP via DHCP reservation"],"related_entries":["github-actions-ssh-timeout-port-22"],"date_context":"2026-02"},{"id":"elastic-ip-needed-for-stable-host","title":"Public IP changing broke deploy until stable endpoint was used","category":"AWS","tags":["elastic-ip","stability"],"summary":"Deploy automation breaks when the endpoint changes.","narration":"If your public IP changes, your pipeline breaks. The fix is a stable endpoint. On AWS that is Elastic IP. On home infra that is a tunnel or Tailscale. Same concept. Stable target.","symptoms":["deploy worked then stopped after restart","host changed"],"root_cause":"Dynamic public IP changed.","fix_steps":[{"step":"Use a stable endpoint concept","why":"Automation needs a consistent target."}],"commands":[],"verification_steps":["Pipeline keeps working after restarts"],"prevention":["Prefer stable endpoints always"],"related_entries":["github-actions-ssh-timeout-port-22"],"date_context":"2025-2026"}]')}}]);